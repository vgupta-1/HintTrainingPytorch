{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Knowledge Distillation with Hint Training: Experiment 1\n",
        "\n",
        "Hint and Guided layer in beginning, middle and end.\n"
      ],
      "metadata": {
        "id": "IWgolZ-tdkau"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UvlphObFDDi1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "\n",
        "mean = torch.tensor([0.4914, 0.4822, 0.4465])\n",
        "std = torch.tensor([0.2009, 0.2009, 0.2009])\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = mean, std = std)])\n",
        "cifar_train_data = datasets.CIFAR100('./data', train=True, download=True, transform=transform)\n",
        "cifar_test_data = datasets.CIFAR100('./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar_train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(cifar_test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4y2M7k_yyyM",
        "outputId": "8b355aea-7ecd-4577-c374-9259b47186c7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_loader:\n",
        "    print(x.shape)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VIc_TCekDs2w",
        "outputId": "0073d44e-a2f9-4273-c37f-1942ea299db7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([128, 3, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network initializations"
      ],
      "metadata": {
        "id": "ZkoqqrFedp3W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JPpZxkpuUXj9"
      },
      "outputs": [],
      "source": [
        "# Wide Shallow neural network class to be used as teacher:\n",
        "# 3 different instantiations where the hint layer parameters are returned, either first, second, or third convolutional layer\n",
        "\n",
        "class DeepNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(DeepNN, self).__init__()\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 356, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            nn.Conv2d(356, 356, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(3204, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "        conv_feature_map = x\n",
        "        x = self.features2(x)\n",
        "        x = self.features3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, conv_feature_map\n",
        "\n",
        "class DeepNN2(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(DeepNN2, self).__init__()\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 356, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            nn.Conv2d(356, 356, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(3204, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "        x = self.features2(x)\n",
        "        conv_feature_map = x\n",
        "        x = self.features3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, conv_feature_map\n",
        "\n",
        "class DeepNN3(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(DeepNN3, self).__init__()\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features2 = nn.Sequential(\n",
        "            nn.Conv2d(128, 356, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features3 = nn.Sequential(\n",
        "            nn.Conv2d(356, 356, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(3204, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "        x = self.features2(x)\n",
        "        x = self.features3(x)\n",
        "        conv_feature_map = x\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, conv_feature_map\n",
        "\n",
        "\n",
        "\n",
        "# Lightweight neural network class to be used as student, much deeper but thinner network:\n",
        "class LightNN(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(LightNN, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(432, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        conv_feature_map = x\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, conv_feature_map"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training and testing loop normal\n",
        "def test(model, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    num = 0\n",
        "    with torch.no_grad():\n",
        "        for (X, y) in test_dl:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred , _ = model(X)\n",
        "            y_pred = torch.argmax(y_pred, dim=1)\n",
        "            correct += torch.eq(y, y_pred).sum()\n",
        "            num += X.shape[0]\n",
        "\n",
        "    accuracy = correct / num\n",
        "    return accuracy.item()\n",
        "\n",
        "def train(model, lr, num_epochs, train_dl, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_accuracy_list = []\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        for X, y in train_dl:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            y_pred, _ = model(X)\n",
        "            loss = loss_fn(y_pred, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "        test_accuracy = test(model, test_dl)\n",
        "        print(f\"Test accuracy at epoch {epoch}: {test_accuracy:.4f}\")\n",
        "        test_accuracy_list.append(test_accuracy)\n",
        "    return test_accuracy_list"
      ],
      "metadata": {
        "id": "unhyppwizpwd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training teacher for 10 epochs\n",
        "lr =  0.001\n",
        "num_epochs = 15\n",
        "teacher = DeepNN()\n",
        "\n",
        "x = train(teacher, lr, num_epochs, train_loader, test_loader)\n",
        "test_accuracy = test(teacher, test_loader)\n",
        "print(f\"Final teacher test accuracy on CIFAR-100: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtBLNNkizyxS",
        "outputId": "d26ab41a-1c9a-45d2-c5df-c305d93ee633"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy at epoch 1: 0.2438\n",
            "Test accuracy at epoch 2: 0.3323\n",
            "Test accuracy at epoch 3: 0.3779\n",
            "Test accuracy at epoch 4: 0.4120\n",
            "Test accuracy at epoch 5: 0.4261\n",
            "Test accuracy at epoch 6: 0.4391\n",
            "Test accuracy at epoch 7: 0.4518\n",
            "Test accuracy at epoch 8: 0.4514\n",
            "Test accuracy at epoch 9: 0.4642\n",
            "Test accuracy at epoch 10: 0.4582\n",
            "Test accuracy at epoch 11: 0.4607\n",
            "Test accuracy at epoch 12: 0.4554\n",
            "Test accuracy at epoch 13: 0.4497\n",
            "Test accuracy at epoch 14: 0.4471\n",
            "Test accuracy at epoch 15: 0.4475\n",
            "Final teacher test accuracy on CIFAR-100: 0.4475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#student model training\n",
        "lr =  0.001\n",
        "num_epochs = 15\n",
        "student = LightNN()\n",
        "\n",
        "x = train(student, lr, num_epochs, train_loader, test_loader)\n",
        "test_accuracy = test(student, test_loader)\n",
        "print(f\"Final student test accuracy on CIFAR-100: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "X-3vcjJPGsgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fedf483-b35b-486a-8374-25e1a92f8a78"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy at epoch 1: 0.0907\n",
            "Test accuracy at epoch 2: 0.1576\n",
            "Test accuracy at epoch 3: 0.1913\n",
            "Test accuracy at epoch 4: 0.2227\n",
            "Test accuracy at epoch 5: 0.2353\n",
            "Test accuracy at epoch 6: 0.2529\n",
            "Test accuracy at epoch 7: 0.2602\n",
            "Test accuracy at epoch 8: 0.2897\n",
            "Test accuracy at epoch 9: 0.3015\n",
            "Test accuracy at epoch 10: 0.3096\n",
            "Test accuracy at epoch 11: 0.3214\n",
            "Test accuracy at epoch 12: 0.3169\n",
            "Test accuracy at epoch 13: 0.3262\n",
            "Test accuracy at epoch 14: 0.3260\n",
            "Test accuracy at epoch 15: 0.3379\n",
            "Final student test accuracy on CIFAR-100: 0.3379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Distillation loss\n",
        "def test_d(model, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    num = 0\n",
        "    with torch.no_grad():\n",
        "        for (X, y) in test_dl:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred, _ = model(X)\n",
        "            y_pred = torch.argmax(y_pred, dim=1)\n",
        "            correct += torch.eq(y, y_pred).sum()\n",
        "            num += X.shape[0]\n",
        "\n",
        "    accuracy = correct / num\n",
        "    return accuracy.item()\n",
        "\n",
        "def train_d(teacher, student, lr, num_epochs, temperature, alpha, train_dl, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    opt = torch.optim.Adam(student.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_accuracy_list = []\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        student.train()\n",
        "        running_loss = 0.0\n",
        "        for X, y in train_dl:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            student_logits, _ = student(X)\n",
        "            with torch.no_grad():\n",
        "                teacher_logits, _ = teacher(X)\n",
        "\n",
        "            student_probs = torch.softmax(student_logits / temperature, dim=1)\n",
        "\n",
        "\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
        "            soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (temperature**2)\n",
        "            label_loss = loss_fn(student_logits, y)\n",
        "            loss = 0.25 * soft_targets_loss + 0.75* label_loss\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        #print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        test_accuracy = test(student, test_dl)\n",
        "        print(f\"Test accuracy at epoch {epoch}: {test_accuracy:.4f}\")\n",
        "        test_accuracy_list.append(test_accuracy)\n",
        "    return test_accuracy_list"
      ],
      "metadata": {
        "id": "H-cqqeJX0k_y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr =  0.001\n",
        "num_epochs = 15\n",
        "student = LightNN()\n",
        "\n",
        "train_d(teacher, student, lr, num_epochs, 2, .75, train_loader, test_loader)\n",
        "test_accuracy = test_d(student, test_loader)\n",
        "print(f\"Final student test accuracy on CIFAR-10: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Swo_bSh3nM9",
        "outputId": "c4dba5a0-0571-4d76-f1d2-11a9ad72b0cc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy at epoch 1: 0.1024\n",
            "Test accuracy at epoch 2: 0.1761\n",
            "Test accuracy at epoch 3: 0.2164\n",
            "Test accuracy at epoch 4: 0.2459\n",
            "Test accuracy at epoch 5: 0.2665\n",
            "Test accuracy at epoch 6: 0.2826\n",
            "Test accuracy at epoch 7: 0.2928\n",
            "Test accuracy at epoch 8: 0.3093\n",
            "Test accuracy at epoch 9: 0.3190\n",
            "Test accuracy at epoch 10: 0.3180\n",
            "Test accuracy at epoch 11: 0.3420\n",
            "Test accuracy at epoch 12: 0.3456\n",
            "Test accuracy at epoch 13: 0.3456\n",
            "Test accuracy at epoch 14: 0.3484\n",
            "Test accuracy at epoch 15: 0.3574\n",
            "Final student test accuracy on CIFAR-10: 0.3574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "i3nziVFLUXkD"
      },
      "outputs": [],
      "source": [
        "class ModifiedLightNNRegressor(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ModifiedLightNNRegressor, self).__init__()\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.features2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features3= nn.Sequential(\n",
        "            nn.Conv2d(32, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
        "        )\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(16, 128, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(432, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "        regressor_output = self.regressor(x)\n",
        "        x = self.features2(x)\n",
        "        x = self.features3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, regressor_output\n",
        "\n",
        "\n",
        "\n",
        "class ModifiedLightNNRegressor2(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ModifiedLightNNRegressor2, self).__init__()\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.features2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features3= nn.Sequential(\n",
        "            nn.Conv2d(32, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
        "        )\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(32, 356, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(432, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "        x = self.features2(x)\n",
        "        regressor_output = self.regressor(x)\n",
        "        x = self.features3(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, regressor_output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ModifiedLightNNRegressor3(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super(ModifiedLightNNRegressor3, self).__init__()\n",
        "        self.features1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.features2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "        self.features3= nn.Sequential(\n",
        "            nn.Conv2d(32, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(48, 48, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=4, stride=2),\n",
        "        )\n",
        "\n",
        "        self.regressor = nn.Sequential(\n",
        "            nn.Conv2d(48, 356, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(432, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features1(x)\n",
        "        x = self.features2(x)\n",
        "        x = self.features3(x)\n",
        "        regressor_output = self.regressor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x, regressor_output"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Distillation loss\n",
        "def test_d(model, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    num = 0\n",
        "    with torch.no_grad():\n",
        "        for (X, y) in test_dl:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred, _ = model(X)\n",
        "            y_pred = torch.argmax(y_pred, dim=1)\n",
        "            correct += torch.eq(y, y_pred).sum()\n",
        "            num += X.shape[0]\n",
        "\n",
        "    accuracy = correct / num\n",
        "    return accuracy.item()\n",
        "\n",
        "def train_d1(teacher, student, lr, num_epochs, temperature, alpha, train_dl, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    opt = torch.optim.Adam(student.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    mse_loss = nn.MSELoss()\n",
        "\n",
        "    test_accuracy_list = []\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        student.train()\n",
        "        running_loss = 0.0\n",
        "        for X, y in train_dl:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            student_logits, regressor_feature_map = student(X)\n",
        "            with torch.no_grad():\n",
        "                teacher_logits, teacher_feature_map = teacher(X)\n",
        "\n",
        "            hidden_rep_loss = mse_loss(regressor_feature_map, teacher_feature_map)\n",
        "            label_loss = loss_fn(student_logits, y)\n",
        "            loss = 0.25 * hidden_rep_loss + 0.75 * label_loss\n",
        "\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        #print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        test_accuracy = test(student, test_dl)\n",
        "        print(f\"Test accuracy at epoch {epoch}: {test_accuracy:.4f}\")\n",
        "        test_accuracy_list.append(test_accuracy)\n",
        "    return test_accuracy_list\n",
        "\n",
        "\n",
        "# Initialize a ModifiedLightNNRegressor\n",
        "torch.manual_seed(42)\n",
        "modified_nn_light_reg = ModifiedLightNNRegressor(num_classes=100).to(device)\n",
        "modified_nn_light_reg2 = ModifiedLightNNRegressor2(num_classes=100).to(device)\n",
        "modified_nn_light_reg3 = ModifiedLightNNRegressor3(num_classes=100).to(device)\n",
        "\n",
        "# We do not have to train the modified deep network from scratch of course, we just load its weights from the trained instance\n",
        "modified_nn_deep_reg = DeepNN(num_classes=100).to(device)\n",
        "modified_nn_deep_reg.load_state_dict(teacher.state_dict())\n",
        "modified_nn_deep_reg2 = DeepNN2(num_classes=100).to(device)\n",
        "modified_nn_deep_reg2.load_state_dict(teacher.state_dict())\n",
        "modified_nn_deep_reg3 = DeepNN3(num_classes=100).to(device)\n",
        "modified_nn_deep_reg3.load_state_dict(teacher.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiFhoqkyoTcW",
        "outputId": "af22e5b1-80f9-42eb-f36b-4dcb71480f37"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for param in modified_nn_light_reg.classifier.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in modified_nn_light_reg2.classifier.parameters():\n",
        "  param.requires_grad = False\n",
        "for param in modified_nn_light_reg3.classifier.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Train and test once again\n",
        "train_d1(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, lr=0.001, num_epochs=10, temperature=0, alpha = 0, train_dl=train_loader, test_dl=test_loader)\n",
        "train_d1(teacher=modified_nn_deep_reg2, student=modified_nn_light_reg2, lr=0.001, num_epochs=10, temperature=0, alpha = 0, train_dl=train_loader, test_dl=test_loader)\n",
        "train_d1(teacher=modified_nn_deep_reg3, student=modified_nn_light_reg3, lr=0.001, num_epochs=10, temperature=0, alpha = 0, train_dl=train_loader, test_dl=test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXxePnJdH8SV",
        "outputId": "a4bbc964-a7df-4859-e76b-379e4d6248c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy at epoch 1: 0.0245\n",
            "Test accuracy at epoch 2: 0.0688\n",
            "Test accuracy at epoch 3: 0.1067\n",
            "Test accuracy at epoch 4: 0.1214\n",
            "Test accuracy at epoch 5: 0.1375\n",
            "Test accuracy at epoch 6: 0.1550\n",
            "Test accuracy at epoch 7: 0.1680\n",
            "Test accuracy at epoch 8: 0.1735\n",
            "Test accuracy at epoch 9: 0.1833\n",
            "Test accuracy at epoch 10: 0.1938\n",
            "Test accuracy at epoch 1: 0.0566\n",
            "Test accuracy at epoch 2: 0.0883\n",
            "Test accuracy at epoch 3: 0.1189\n",
            "Test accuracy at epoch 4: 0.1353\n",
            "Test accuracy at epoch 5: 0.1471\n",
            "Test accuracy at epoch 6: 0.1613\n",
            "Test accuracy at epoch 7: 0.1729\n",
            "Test accuracy at epoch 8: 0.1829\n",
            "Test accuracy at epoch 9: 0.1945\n",
            "Test accuracy at epoch 10: 0.1939\n",
            "Test accuracy at epoch 1: 0.0337\n",
            "Test accuracy at epoch 2: 0.0721\n",
            "Test accuracy at epoch 3: 0.1027\n",
            "Test accuracy at epoch 4: 0.1173\n",
            "Test accuracy at epoch 5: 0.1286\n",
            "Test accuracy at epoch 6: 0.1431\n",
            "Test accuracy at epoch 7: 0.1490\n",
            "Test accuracy at epoch 8: 0.1602\n",
            "Test accuracy at epoch 9: 0.1703\n",
            "Test accuracy at epoch 10: 0.1893\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.03370000049471855,\n",
              " 0.07209999859333038,\n",
              " 0.10269999504089355,\n",
              " 0.11729999631643295,\n",
              " 0.12860000133514404,\n",
              " 0.14309999346733093,\n",
              " 0.14899998903274536,\n",
              " 0.16019999980926514,\n",
              " 0.17029999196529388,\n",
              " 0.18930000066757202]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unfreeze parameters and train finally with knowledge distillation for stage 2\n",
        "\n",
        "for param in modified_nn_light_reg.classifier.parameters():\n",
        "  param.requires_grad = True\n",
        "for param in modified_nn_light_reg2.classifier.parameters():\n",
        "  param.requires_grad = True\n",
        "for param in modified_nn_light_reg3.classifier.parameters():\n",
        "  param.requires_grad = True\n",
        "\n",
        "\n",
        "def test_d(model, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    num = 0\n",
        "    with torch.no_grad():\n",
        "        for (X, y) in test_dl:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            y_pred, _ = model(X)\n",
        "            y_pred = torch.argmax(y_pred, dim=1)\n",
        "            correct += torch.eq(y, y_pred).sum()\n",
        "            num += X.shape[0]\n",
        "\n",
        "    accuracy = correct / num\n",
        "    return accuracy.item()\n",
        "\n",
        "def train_d(teacher, student, lr, num_epochs, temperature, alpha, train_dl, test_dl):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    teacher.to(device)\n",
        "    student.to(device)\n",
        "    opt = torch.optim.Adam(student.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    test_accuracy_list = []\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        student.train()\n",
        "        running_loss = 0.0\n",
        "        for X, y in train_dl:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            opt.zero_grad()\n",
        "            student_logits, _ = student(X)\n",
        "            with torch.no_grad():\n",
        "                teacher_logits, _ = teacher(X)\n",
        "\n",
        "            student_probs = torch.softmax(student_logits / temperature, dim=1)\n",
        "\n",
        "\n",
        "            soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=-1)\n",
        "            soft_prob = nn.functional.log_softmax(student_logits / temperature, dim=-1)\n",
        "            soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (temperature**2)\n",
        "            label_loss = loss_fn(student_logits, y)\n",
        "            loss = 0.25 * soft_targets_loss + 0.75* label_loss\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        #print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "        test_accuracy = test(student, test_dl)\n",
        "        print(f\"Test accuracy at epoch {epoch}: {test_accuracy:.4f}\")\n",
        "        test_accuracy_list.append(test_accuracy)\n",
        "    return test_accuracy_list"
      ],
      "metadata": {
        "id": "pC4lKIqPOfQe"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_d(teacher=modified_nn_deep_reg, student=modified_nn_light_reg, lr=0.001, num_epochs=10, temperature=2, alpha = 0.75, train_dl=train_loader, test_dl=test_loader)\n",
        "train_d(teacher=modified_nn_deep_reg2, student=modified_nn_light_reg2,  lr=0.001, num_epochs=10, temperature=2, alpha = 0.75, train_dl=train_loader, test_dl=test_loader)\n",
        "train_d(teacher=modified_nn_deep_reg3, student=modified_nn_light_reg3,  lr=0.001, num_epochs=10, temperature=2, alpha = 0.75, train_dl=train_loader, test_dl=test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnxFh-JIP__p",
        "outputId": "b33b3a51-e06d-4eeb-dcc8-4561952d6a48"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy at epoch 1: 0.2553\n",
            "Test accuracy at epoch 2: 0.2847\n",
            "Test accuracy at epoch 3: 0.3084\n",
            "Test accuracy at epoch 4: 0.3200\n",
            "Test accuracy at epoch 5: 0.3347\n",
            "Test accuracy at epoch 6: 0.3383\n",
            "Test accuracy at epoch 7: 0.3509\n",
            "Test accuracy at epoch 8: 0.3597\n",
            "Test accuracy at epoch 9: 0.3662\n",
            "Test accuracy at epoch 10: 0.3661\n",
            "Test accuracy at epoch 1: 0.2707\n",
            "Test accuracy at epoch 2: 0.3010\n",
            "Test accuracy at epoch 3: 0.3183\n",
            "Test accuracy at epoch 4: 0.3350\n",
            "Test accuracy at epoch 5: 0.3442\n",
            "Test accuracy at epoch 6: 0.3623\n",
            "Test accuracy at epoch 7: 0.3638\n",
            "Test accuracy at epoch 8: 0.3751\n",
            "Test accuracy at epoch 9: 0.3806\n",
            "Test accuracy at epoch 10: 0.3723\n",
            "Test accuracy at epoch 1: 0.2432\n",
            "Test accuracy at epoch 2: 0.2807\n",
            "Test accuracy at epoch 3: 0.2922\n",
            "Test accuracy at epoch 4: 0.3093\n",
            "Test accuracy at epoch 5: 0.3261\n",
            "Test accuracy at epoch 6: 0.3388\n",
            "Test accuracy at epoch 7: 0.3457\n",
            "Test accuracy at epoch 8: 0.3568\n",
            "Test accuracy at epoch 9: 0.3568\n",
            "Test accuracy at epoch 10: 0.3602\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.24319998919963837,\n",
              " 0.2806999981403351,\n",
              " 0.2921999990940094,\n",
              " 0.3093000054359436,\n",
              " 0.3260999917984009,\n",
              " 0.33879998326301575,\n",
              " 0.3456999957561493,\n",
              " 0.35679998993873596,\n",
              " 0.35679998993873596,\n",
              " 0.3601999878883362]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LC9GhxKx4PL_"
      },
      "outputs": [],
      "source": [
        "def test_multiple_outputs(model, test_loader, device):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs, _ = model(inputs) # Disregard the second tensor of the tuple\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Teacher Accuracy:\")\n",
        "test_accuracy_deep = test(modified_nn_deep_reg, test_loader)\n",
        "print(f\"{(100*test_accuracy_deep):.2f}%\")\n",
        "\n",
        "test_accuracy = test_d(student, test_loader)\n",
        "print(f\"Final student accuracy KD: {test_accuracy:.4f}\")\n",
        "\n",
        "print(f\"Final Student Accuracy (Hint = 1):\")\n",
        "test_accuracy_light_ce_and_mse_loss1 = test_multiple_outputs(modified_nn_light_reg, test_loader, device)\n",
        "print(f\"Final Student Accuracy (Hint = 2):\")\n",
        "test_accuracy_light_ce_and_mse_loss2 = test_multiple_outputs(modified_nn_light_reg2, test_loader, device)\n",
        "print(f\"Final Student Accuracy (Hint = 3):\")\n",
        "test_accuracy_light_ce_and_mse_loss3 = test_multiple_outputs(modified_nn_light_reg3, test_loader, device)\n"
      ],
      "metadata": {
        "id": "t97x9NPtYgLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3dcc84b-bd23-49ba-8fc7-0e297e49796c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher Accuracy:\n",
            "44.75%\n",
            "Final student accuracy KD: 0.3574\n",
            "Final Student Accuracy (Hint = 1):\n",
            "Test Accuracy: 36.61%\n",
            "Final Student Accuracy (Hint = 2):\n",
            "Test Accuracy: 37.23%\n",
            "Final Student Accuracy (Hint = 3):\n",
            "Test Accuracy: 36.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn_light = LightNN(num_classes=100).to(device)\n",
        "total_params_deep = \"{:,}\".format(sum(p.numel() for p in teacher.parameters()))\n",
        "print(f\"DeepNN parameters: {total_params_deep}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in student.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in modified_nn_light_reg.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in modified_nn_light_reg2.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")\n",
        "total_params_light = \"{:,}\".format(sum(p.numel() for p in modified_nn_light_reg3.parameters()))\n",
        "print(f\"LightNN parameters: {total_params_light}\")"
      ],
      "metadata": {
        "id": "6HVR6AyHYsjd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4733320c-67a9-4062-b246-2cade4be6e15"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepNN parameters: 3,247,292\n",
            "LightNN parameters: 151,988\n",
            "LightNN parameters: 170,548\n",
            "LightNN parameters: 254,872\n",
            "LightNN parameters: 306,136\n"
          ]
        }
      ]
    }
  ]
}