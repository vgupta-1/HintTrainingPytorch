# HintTrainingPytorch

This is a project exploring the hint training techniques brought forth in FITNETS: HINTS FOR THIN DEEP NETS from Romero et. al. The of this project is to use the two-stage knowledge distillation process in this paper to create smaller models that can perform as well as a large teacher model. In Romero et. al the did not consider the hint and guided layer locations except for the middle of the networks, so I run an experiment to see what happens in performance if the hint and guided layers are also in the beginning and end of the networks. I run another experiment experimenting to see whether student depth improves performance. I also run an experiment seeing whether having multiple hint and guided layers in training improve the performance of the student model. Overall, this project is an exploration of the hint training techniquies brought forth in FITNETS: HINTS FOR THIN DEEP NETS and trying some concepts not brought forth in the paper. 
